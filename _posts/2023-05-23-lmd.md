---
layout: post
title: "GPT-4 + Stable-Diffusion = ?: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models"
date: 2023-05-23  15:00:00
author: <a href="https://tonylian.com/">Long Lian</a>, <a href="https://sites.google.com/site/boyilics/home">Boyi Li</a>, <a href="https://www.adamyala.org/">Adam Yala</a>, and <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
img: /assets/lmd/main.jpg
excerpt_separator: <!--more-->
visible: True
show_comments: False
---

<!-- twitter -->
<meta name="twitter:title" content="GPT-4 + Stable-Diffusion = ?: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="assets/lmd/main.jpg">

<meta name="keywords" content="gpt-4, stable diffusion, llm, text-to-image, diffusion models, large language models, prompt understanding">
<meta name="description" content="The BAIR Blog">
<meta name="author" content="Long Lian, Boyi Li, Adam Yala, Trevor Darrell">

**TL;DR**: Text Prompt -> LLM -> Intermediate Representation (such as an image layout) -> Stable Diffusion -> Image.

Recent advancements in text-to-image generation with diffusion models have yielded remarkable results synthesizing highly realistic and diverse images. However, despite their impressive capabilities, diffusion models, such as [Stable Diffusion](https://arxiv.org/abs/2112.10752), often struggle to accurately follow the prompts when spatial or common sense reasoning is required.

The following figure lists four scenarios in which Stable Diffusion falls short in generating images that accurately correspond to the given prompts, namely **negation**, **numeracy**, and **attribute assignment**, **spatial relationships**. In contrast, our method, **L**L**M**-grounded **D**iffusion (**LMD**), delivers much better prompt understanding in text-to-image generation in those scenarios.

<p style="text-align:center">
<img src="https://bair.berkeley.edu/static/blog/lmd/visualizations.jpg" alt="Visualizations" width="95%">
<b><i>Figure 1: LLM-grounded Diffusion enhances the prompt understanding ability of text-to-image diffusion models.</i></b>
</p>

<!--more-->

One possible solution to address this issue is of course to gather a vast multi-modal dataset comprising intricate captions and train a large diffusion model with a large language encoder. This approach comes with significant costs: It is time-consuming and expensive to train both large language models (LLMs) and diffusion models.

## Our Solution

To efficiently solve this problem with minimal cost (i.e., no training costs), we instead **equip diffusion models with enhanced spatial and common sense reasoning by using off-the-shelf frozen LLMs** in a novel two-stage generation process.

First, we adapt an LLM to be a text-guided layout generator through in-context learning. When provided with an image prompt, an LLM outputs a scene layout in the form of bounding boxes along with corresponding individual descriptions. Second, we steer a diffusion model with a novel controller to generate images conditioned on the layout. Both stages utilize frozen pretrained models without any LLM or diffusion model parameter optimization. We invite readers to [read the paper on arXiv](https://arxiv.org/pdf/2305.13655.pdf) for additional details.

<p style="text-align:center">
<img src="https://bair.berkeley.edu/static/blog/lmd/main.jpg" alt="Text to layout" width="95%">
<b><i>Figure 2: LMD is a text-to-image generative model with a novel two-stage generation process: a text-to-layout generator with an LLM + in-context learning and a novel layout-guided stable diffusion. Both stages are training-free.</i></b>
</p>

## LMD's Additional Capabilities

Additionally, LMD naturally allows **dialog-based multi-round scene specification**, enabling additional clarifications and subsequent modifications for each prompt. Furthermore, LMD is able to **handle prompts in a language that is not well-supported by the underlying diffusion model**.

<p style="text-align:center">
<img src="https://bair.berkeley.edu/static/blog/lmd/additional_abilities.jpg" alt="Additional abilities" width="95%">
<b><i>Figure 3: Incorporating an LLM for prompt understanding, our method is able to perform dialog-based scene specification and generation from prompts in a language (Chinese in the example above) that the underlying diffusion model does not support.</i></b>
</p>

Given an LLM that supports multi-round dialog (e.g., GPT-3.5 or GPT-4), LMD allows the user to provide additional information or clarifications to the LLM by querying the LLM after the first layout generation in the dialog and generate images with the updated layout in the subsequent response from the LLM. For example, a user could request to add an object to the scene or change the existing objects in location or descriptions (the left half of Figure 3).

Furthermore, by giving an example of a non-English prompt with a layout and background description in English during in-context learning, LMD accepts inputs of non-English prompts and will generate layouts, with descriptions of boxes and the background in English for subsequent layout-to-image generation. As shown in the right half of Figure 3, this allows generation from prompts in a language that the underlying diffusion models do not support.

## Visualizations

We validate the superiority of our design by comparing it with the base diffusion model (SD 2.1) that LMD uses under the hood. We invite readers to our work for more evaluation and comparisons.

<p style="text-align:center">
<img src="https://bair.berkeley.edu/static/blog/lmd/visualizations_main.jpg" alt="Main Visualizations" width="95%">
<b><i>Figure 4: LMD outperforms the base diffusion model in accurately generating images according to prompts that necessitate both language and spatial reasoning. LMD also enables counterfactual text-to-image generation that the base diffusion model is not able to generate (the last row).</i></b>
</p>

For more details about LLM-grounded Diffusion (LMD), [visit our website](https://llm-grounded-diffusion.github.io) and [read the paper on arXiv](https://arxiv.org/pdf/2305.13655.pdf).

## BibTex

If LLM-grounded Diffusion inspires your work, please cite it with:

```
@article{lian2023llmgrounded,
    title={LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models},
    author={Lian, Long and Li, Boyi and Yala, Adam and Darrell, Trevor},
    journal={arXiv preprint arXiv:2305.13655},
    year={2023}
}
```
