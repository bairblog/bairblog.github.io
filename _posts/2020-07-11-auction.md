---
layout:             post
title:              "Decentralized Reinforcement Learning:<br>Global Decision-Making via<br>Local Economic Transactions"
date:               2020-07-11 9:00:00
author:             <a href="http://mbchang.github.io/">Michael Chang</a> and <a href="https://www.linkedin.com/in/sid-k-232763a6/">Sidhant Kaushik</a>
img:                assets/auction/mnist.png
excerpt_separator:  <!--more-->
visible:            True
show_comments:      False
---

<article class="post-content">
<meta name="twitter:title" content="Global Decision-Making via Local Economic Transactions">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://bair.berkeley.edu/static/blog/auction/mnist.png">

<!-- begin section I: introduction -->

<p>
Many neural network architectures that underlie various artificial intelligence systems today bear an interesting similarity to the early computers a century ago.
Just as early computers were specialized circuits for specific purposes like <a href="http://jva.cs.iastate.edu/operation.php">solving linear systems</a> or <a href="https://en.wikipedia.org/wiki/Colossus_computer">cryptanalysis</a>, so too does the trained neural network generally function as a <a href="https://youtu.be/9EN_HoEk3KY?t=211">specialized circuit</a> for performing a specific task, with all parameters coupled together in the same global scope.
</p>

<p>
One might naturally wonder what it might take for <i>learning</i> systems to scale in complexity in the same way as <i>programmed</i> systems have.
And if the history of <a href="https://www.youtube.com/watch?v=qAKrMdUycb8">how abstraction enabled computer science to scale</a> gives any indication, one possible place to start would be to consider what it means to build complex learning systems at multiple levels of abstraction, where each level of learning is the emergent consequence of learning from the layer below.
</p>

<p>
This post discusses <a href="https://arxiv.org/abs/2007.02382">our recent paper</a> that introduces a framework for <b>societal decision-making</b>, a perspective on reinforcement learning through the lens of a self-organizing society of primitive agents.
We prove the optimality of an incentive mechanism for engineering the society to optimize a collective objective.
Our work also provides suggestive evidence that the local credit assignment scheme of the <b>decentralized reinforcement learning algorithms</b> we develop to train the society facilitates more efficient transfer to new tasks.
</p>

<!--more-->

<h2 id="levels-of-abstraction-in-complex-learning-systems">Levels of Abstraction in Complex Learning Systems</h2>

<p>
From corporations to organisms, <a href="https://youtu.be/uyUbGatPKpI?t=1419">many large-scale systems in our world are composed of smaller individual autonomous components</a>, whose collective function serve a larger objective than the objective of any individual component alone.
A corporation for example, optimizes for profits as if it were a single super-agent when in reality it is a society of self-interested human agents, each with concerns that may have little to do with profit.
And every human is also simply an abstraction of organs, tissues, and cells individually adapting and making their own simpler decisions.
</p>

<blockquote cite="http://aurellem.org/society-of-mind/som-1.3.html">
<p>You know that everything you think and do is thought and done by you. But what's a "you"? What kinds of smaller entities cooperate inside your mind to do your work?</p>
<p align="right">&mdash; Marvin Minsky, <cite>The Society of Mind</cite></p>
</blockquote>

<p>
At the core of building complex learning systems at multiple levels of abstraction is to understand the mechanisms that bind consecutive levels together.
In the context of learning for decision-making, this means to define three ingredients:
</p>
<ul id="ingredients">
    <li>A <b>framework</b> for expressing the encapsulation of a society of primitive agents as a super-agent</li>
    <li>An <b>incentive mechanism</b> that guarantees the optimal solution for the super-agent's decision problem emerges as a consequence of the primitives optimizing their individual decision problems</li>
    <li>A <b>learning algorithm</b> for implicitly training the super-agent by directly training the primitives</li>
</ul>
<p>
The incentive mechanism is the abstraction barrier that connects the optimization problems of the primitive agents from the optimization problem of the society as a super-agent.
</p>


<p style="text-align:center;">
    <img src="https://bair.berkeley.edu/static/blog/auction/abstraction_barrier.png" width="100%">
    <br>
    <i>Building complex learning systems at multiple levels of abstraction requires defining the incentive mechanism that connects the optimization problems at the level of primitive agent to the optimization problem at the level of the society. The incentive mechanism is the abstraction barrier that separates the society as a super-agent from its constituent primitive agents.</i>
</p>

<p>
If it were possible to construct the incentive mechanism in a way that the <a href="https://en.wikipedia.org/wiki/Strategic_dominance#Dominance_and_Nash_equilibria">dominant strategy equilibrium</a> of the primitive agents coincides with the optimal solution for the super-agent, then the society can in theory be faithfully abstracted as a super-agent, which could then serve as a primitive for the next level of abstraction, and so on, thereby constructing in a learning system the higher and higher levels of complexity that characterize the programmed systems of modern software infrastructure.
</p>

<!-- begin section II: model-based techniques -->

<!-- <h2 id="global-decision-making-via-local-economic-transactions">Global Decision-Making via Local Economic Transactions</h2>
 -->

<h2 id="a-market-economy-perspective-on-reinforcement-learning">A Market Economy Perspective on Reinforcement Learning</h2>

<p>
As a first step towards this goal, we can work backwards: start with an agent, imagine it were a super-agent, and study how to emulate optimal behavior of such an agent via a society of even more primitive agents.
We consider a restricted scenario that builds upon existing frameworks familiar to us, <a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov decision processes (MDP)</a>.
Normally, the objective of the learner is to maximize the <a href="https://en.wikipedia.org/wiki/Markov_decision_process#Optimization_objective">expected return</a> of the MDP.
In deep reinforcement learning, the approach that directly optimizes this objective parameterizes the policy as a function that maps states to actions and adjusts the policy parameters according to the gradient of the MDP objective.
</p>

<p>
We refer to this standard approach as the <strong>monolithic decision-making framework</strong> because all the learnable parameters are globally coupled together under a single objective.
The monolithic decision-making framework views reinforcement learning from the perspective of a <strong><a href="https://en.wikipedia.org/wiki/Planned_economy">command economy</a></strong>, in which all production &mdash; the transformation of past states $s_t$ into future states $s_{t+1}$ &mdash; and wealth distribution &mdash; the credit assignment of reward signals to parameters &mdash; derive directly from single central authority &mdash; the MDP objective.
</p>

<p style="text-align:center;">
    <img src="https://bair.berkeley.edu/static/blog/auction/centralized_gif.gif" width="100%">
    <br>
    <i>In the monolithic decision-making framework, actions are chosen passively by the agent.</i>
</p>

<p>
But as suggested <a href="https://pdfs.semanticscholar.org/4373/526d6a46bac9c2de569557957d0b052a437a.pdf?_ga=2.198702767.1414080719.1594348633-1342068630.1594256465">in</a> previous <a href="http://people.idsia.ch/~juergen/economy.html">work</a> dating <a href="https://dl.acm.org/doi/10.5555/645511.657087">back</a> at least two decades, we can also view reinforcement learning from the perspective of a <strong><a href="https://en.wikipedia.org/wiki/Market_economy">market economy</a></strong>, in which production and wealth distribution are governed by the economic transactions between actions that <i>buy and sell states to each other.</i>
Rather than being passively chosen by a global policy as in the monolithic framework, the actions are primitive agents that actively choose <i>themselves</i> when to activate in the environment by bidding in an auction to transform the state $s_t$ to the next state $s_{t+1}$.
We call this the <strong>societal decision-making</strong> framework because these actions form a society of primitive agents that themselves seek to maximize their auction utility at each state.
In other words, the society of primitive agents form a super-agent that solves the MDP as a consequence of the primitive agents' optimal auction strategies.
</p>

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/auction/decentralized_gif.gif" width="100%">
<br>
<i>In the societal decision-making framework, actions actively choose themselves when to activate.</i>
</p>

<!-- <blockquote>
<p>Wealth is distributed based on what future primitives decide to bid for the fruits of the labor of information processing carried out by past primitives transforming one state to another.</p>
</blockquote> -->

<p>
In <a href="https://arxiv.org/abs/2007.02382">our recent work</a>, we formalize the societal decision-making framework and develop a class of <b>decentralized reinforcement learning algorithms</b> for optimizing the super-agent as a by-product of optimizing the primitives' auction utilities.
We show that adapting the <a href="https://en.wikipedia.org/wiki/Vickrey_auction">Vickrey auction</a> as the auction mechanism and initializing redundant clones of each primitive yields a society, which we call the <strong>cloned Vickrey society</strong>, whose dominant strategy equilibrium of the primitives optimizing their auction utilities coincides with the optimal policy of the super-agent the society collectively represents.
In particular, with the following specification of auction utility, we can leverage the <a href="https://en.wikipedia.org/wiki/Strategyproofness">truthfulness</a> property of the Vickrey auction to incentivize the primitive agents, which we denote by $\omega^{1:N}$, to bid the optimal Q-value of their corresponding action:
</p>

<p style="text-align:center;">
    <img src="https://bair.berkeley.edu/static/blog/auction/utility.png" width="75%">
    <br>
    <i>The utility $\hat{U}^i_{s_t}$ for the primitive with the highest bid, $\hat{\omega}^i$ is given by the revenue it receives from selling $s_{t+1}$ in the auction at the next time-step minus the price $\max_{j \neq i} \mathbf{b}^j_{s_t}$ it pays for buying $s_t$ from the auction winner at the previous time-step. The revenue is given by the environment reward $r(s_t, \hat{\omega}^i)$ plus the discounted highest bid $\max_k \mathbf{b}^k_{s_{t+1}}$ at the next time-step. In accordance with the Vickrey auction, the price is given by the second highest bid at the current time-step. The utility of losing agents is $0$.</i>
</p>

<p>
The revenue that the winning primitive receives for producing $s_{t+1}$ from $s_t$ depends on the price the winning primitive at $t+1$ is willing to bid for $s_{t+1}$.
In turn, the winning primitive at $t+1$ sells $s_{t+2}$ to the winning primitive at $t+2$, and so on.
Ultimately currency is grounded in the environment reward.
Wealth is distributed based on what future primitives decide to bid for the fruits of the labor of information processing carried out by past primitives transforming one state to another.
</p>

<p>
Under the Vickrey auction, the dominant strategy for each primitive is to truthfully bid exactly the revenue it would receive.
With the above utility function, a primitive's truthful bid at equilibrium is the optimal Q-value of its corresponding action.
And since the primitive with the maximum bid in the auction gets to take its associated action in the environment, overall the society at equilibrium activates the agent with the highest optimal Q-value &mdash; the optimal policy of the super agent.
Thus in the restricted setting we consider, the societal decision-making framework, the cloned Vickrey society, and the decentralized reinforcement learning algorithms provide answers to the <a href="#ingredients">three ingredients</a> outlined above for relating the learning problem of the primitive agent to the learning problem of the society.
</p>

<p>
Societal decision-making frames standard reinforcement learning from the perspective of self-organizing primitive agents.
As we discuss next, the primitive agents need not be restricted to literal actions.
The agents can be any computation that transforms a state from one to another, including <a href="#hrl_fig">options in semi-MDPs</a> or <a href="#mnist">functions in dynamic computation graphs</a>.
</p>

<h3 id="local-credit-assignment-for-more-efficient-transfer">Local Credit Assignment for More Efficient Learning and Transfer</h3>

<p>
Whereas learning in the command economy system of monolithic decision-making requires global credit assignment pathways because all learnable parameters are globally coupled, learning in the market economy system of societal decision-making requires only credit assignment that is local in space and time because the primitives only optimize for their immediate local auction utility without regard to the global learning objective of the society.
Indeed, we find evidence that suggests that the inherent modularity in framing the learning problem of the society in this way offers advantages in transferring to new tasks.
</p>

<p id="hrl_fig" style="text-align:center;">
    <img src="https://bair.berkeley.edu/static/blog/auction/hrl_weight.png" width="90%">
    <br>
    <i>We consider transferring from the pre-training task of reaching the green goal to transfer task of reaching the blue goal in the <a href="https://github.com/maximecb/gym-minigrid">MiniGrid gym environment</a>. $\phi^0$ represents an option that opens the red door, $\phi^1$ represents an option that reaches the blue goal, and $\phi^2$ represents an option that reaches the green goal. The primitive associated with a particular option $\phi^i$ activates by executing that option in the environment. "Credit Conserving Vickrey Cloned" refers to our society-based decentralized reinforcement learning algorithm, which learns much more efficiently than both a hierarchical monolithic baseline equipped to select the same options and a non-hierarchical monolithic baseline that only selects literal actions.
    In particular, we observe that a higher percentage of the hierarchical monolithic baseline's weights have shifted during transfer compared to our method, which suggests that the hierarchical monolithic baseline's weights are more globally coupled and perhaps thereby slower to transfer.
    </i>
</p>

<h3 id="problem-solving-via-analogy">Problem Solving via Analogy</h3>

<blockquote>
<p>Solving a problem simply means representing it so as to make the solution transparent.</p>
<p align="right">&mdash; Herbert Simon, <cite>The Science of Design: Creating the Artificial.</cite></p>
</blockquote>

<p>
<a href="https://mitpress.mit.edu/books/analogy-making-perception">Re-representing an observation as an instance of what is more familiar</a> has been an important topic of study in human cognition from the perspective of <a href="http://bert.stuy.edu/pbrooks/ai/resources/Analogy%20as%20the%20Core%20of%20Cognition-2.pdf">analogy-making</a>.
One particularly intuitive example of this phenomenon are <a href="https://science.sciencemag.org/content/171/3972/701">the mental rotations</a> studied by Roger Shepard that suggested that humans seemed to compose mental rotation operations in their mind for certain types of image recognition.
Inspired by these above works, we considered an image recognition task based on <a href="https://arxiv.org/pdf/1807.04640.pdf">earlier work</a> where we define each primitive agent as representing a different affine transformation.
By using the classification accuracy of an MNIST digit classifier as the sole reward signal, the society of primitives learns to emulate the analogy-making process by iteratively re-representing unfamiliar images into more familiar ones that the classifier knows how to classify.
</p>

<p id="mnist" style="text-align:center;">
    <img src="https://bair.berkeley.edu/static/blog/auction/mnist.png" width="75%">
    <br>
    <i>The society learns to classify transformed digits by making analogies to the digit's canonical counterpart. Here $\omega$ represents a primitive agent, $\psi$ represents that agent's bidding policy, and $\phi$ represents that agent's affine transformation. This figure shows a society with redundant primitives, where clones are indicated by an apostrophe. The benefits of redundancy for robustness are discussed in the paper.</i>
</p>


<h2 id="looking-forward">Looking Forward</h2>

<p>
Modeling intelligence at various levels of abstraction has its roots in the <a href="https://en.wikipedia.org/wiki/Society_of_Mind">early foundations of AI</a>, and modeling the mind as a society of agents goes back as far as <a href="https://en.wikipedia.org/wiki/Republic_(Plato)">Plato's Republic</a>.
In this restricted setting where the primitive agents seek to maximize utility in auctions and the society seeks to maximize return in the MDP, we now have a small piece of the puzzle towards building complex learning systems at multiple levels of abstraction.
There are many more pieces left to go.
</p>
<p>
In some sense these complex learning systems are grown rather than built because every component at every abstraction layer is learning.
But in the same way that programming methodology emerged as a discipline for defining best practices for building complex programmed systems, so too will we need to specify, build, and test the scaffolding that guide the growth of complex learning systems.
This type of deep learning is not only deep in levels of representation but deep in levels of learning.
</p>

<hr>

<p>
    This post is based on the following paper:
</p>

<ul>
<a href="https://arxiv.org/abs/2007.02382"><strong>Decentralized Reinforcement Learning: <br>Global Decision-Making via Local Economic Transactions</strong></a>
<br>
<a href="http://mbchang.github.io/">Michael Chang</a>, <a href="https://www.linkedin.com/in/sid-k-232763a6/">Sidhant Kaushik</a>, <a href="https://www.cs.princeton.edu/~smattw/">S. Matthew Weinberg</a>, <a href="http://cocosci.princeton.edu/tom/index.php">Thomas Griffiths</a>, <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
<br>
<em>Thirty-seventh International Conference Machine Learning (ICML), 2020.</em>
<br>
<a href="https://sites.google.com/view/clonedvickreysociety/home">Webpage</a>
</ul>

<hr>

<p>
    <em>Michael Chang would like to thank Matt Weinberg, Tom Griffiths, and Sergey Levine for their guidance on this project, as well as Michael Janner, Anirudh Goyal, and Sam Toyer for discussions that inspired many of the ideas written here.</em>
</p>


<strong>References</strong>
<font size="-1">
</font><ol><font size="-1">
    <li><a href="https://youtu.be/9EN_HoEk3KY?t=211">Ilya Sutskever: OpenAI Meta-Learning and Self-Play | MIT Artificial General Intelligence (AGI)</a>. 2018.</li>
    <li><a href="https://www.youtube.com/watch?v=qAKrMdUycb8">Barbara Liskov, 2007 ACM A.M. Turing Award Lecture "The Power of Abstraction"</a>. 2013.</li>
    <li><a href="https://www.youtube.com/watch?v=uyUbGatPKpI&feature=youtu.be&t=1419">Social Intelligence | Blaise Aguera y Arcas | NeurIPS 2019</a>. 2020.</li>
    <li>Minsky, Marvin. <a href="http://aurellem.org/society-of-mind/som-1.3.html">The Society of Mind</a>. 1988.</li>
    <li>Baum, Eric B. <a href="https://pdfs.semanticscholar.org/4373/526d6a46bac9c2de569557957d0b052a437a.pdf?_ga=2.198702767.1414080719.1594348633-1342068630.1594256465">Toward a Model of Mind as a Laissez-Faire Economy of Idiots</a>. 1995. </li>
    <li>Schmidhuber, Juergen. <a href="http://people.idsia.ch/~juergen/economy.html">Market Models for Machine Learning - Reinforcement Learning Economies</a>.</li>
    <li>Holland, John H. <a href="https://dl.acm.org/doi/10.5555/645511.657087">Properties of the Bucket Brigade</a>. 1985.</li>
    <li>Simon, Herbert A. <a href="https://www.jstor.org/stable/1511391?casa_token=st5XoO1v6lAAAAAA%3AvHUEwBeEuaIqf6Dp-Aoc2Py8K42qUXeDtO0-ilJWmI1icYlKuevZ9utb2jaDdNcsuvoQjvpVHNrGa_beDtRKApW5UoOHYMIKg7bOmmPTnnG92nzkawlX&seq=1#metadata_info_tab_contents">The Science of Design: Creating the Artificial</a>. 1988.</li>
    <li>Mitchell, Melanie. <a href="https://mitpress.mit.edu/books/analogy-making-perception">Analogy-Making as Perception: A Computer Model</a>. 1993.</li>
    <li>Hofstadter, Douglas R. <a href="http://bert.stuy.edu/pbrooks/ai/resources/Analogy%20as%20the%20Core%20of%20Cognition-2.pdf">Analogy as the core of cognition</a>. 2001.</li>
    <li>Shepard, Roger N., and Jacqueline Metzler. <a href="https://science.sciencemag.org/content/171/3972/701">Mental rotation of three-dimensional objects.</a> 1971.</li>
    <li>Chang, Michael B., et al. <a href="https://arxiv.org/abs/1807.04640">Automatically composing representation transformations as a means for generalization</a>. 2019.</li>

</font></ol><font size="-1">
</font>

</article>
