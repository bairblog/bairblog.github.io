---
layout:             post
title:              "Whole-Body Conditioned Egocentric Video Prediction"
date:               2025-07-01 09:00:00
author:             <a href="https://yutongbai.com/">Yutong Bai</a>, <a href="https://dannytran123.github.io/">Danny Tran</a>, <a href="https://www.amirbar.net/">Amir Bar</a>, <a href="http://yann.lecun.com/">Yann LeCun</a>, <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>, and <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a> <br>
img:                /assets/peva/teaserv3_web.png
excerpt_separator:  <!--more-->
visible:            True
show_comments:      False
---

<!-- Modal for image zoom -->
<style>
.modal {
  display: none;
  position: fixed;
  z-index: 9999;
  padding-top: 50px;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  overflow: auto;
  background-color: rgba(0,0,0,0.9);
}

.modal-content {
  margin: auto;
  display: block;
  max-width: 90%;
  max-height: 90%;
}

.close {
  position: absolute;
  top: 15px;
  right: 35px;
  color: #f1f1f1;
  font-size: 40px;
  font-weight: bold;
  transition: 0.3s;
  cursor: pointer;
}

.close:hover,
.close:focus {
  color: #bbb;
  text-decoration: none;
  cursor: pointer;
}

.clickable-img {
  cursor: zoom-in;
  transition: opacity 0.3s;
}

.clickable-img:hover {
  opacity: 0.9;
}

@media only screen and (max-width: 700px){
  .modal-content {
    width: 100%;
  }
}
</style>

<!-- Modal HTML -->
<div id="imageModal" class="modal">
  <span class="close">&times;</span>
  <img class="modal-content" id="modalImg">
</div>

<script>
document.addEventListener('DOMContentLoaded', function() {
  var modal = document.getElementById('imageModal');
  var modalImg = document.getElementById('modalImg');
  var span = document.getElementsByClassName('close')[0];
  
  // Add click handler to all images in the post
  var images = document.querySelectorAll('.post-content img, article img');
  images.forEach(function(img) {
    // Make all images clickable
    img.classList.add('clickable-img');
    img.title = 'Click to enlarge';
    img.onclick = function() {
      modal.style.display = 'block';
      // Use the original high-res version if it exists
      var highResSrc = this.src.replace('_web.png', '.png');
      modalImg.src = highResSrc;
      modalImg.onerror = function() {
        // Fall back to the web version if high-res doesn't exist
        modalImg.src = img.src;
      };
    }
  });
  
  // Close modal when clicking the X
  span.onclick = function() {
    modal.style.display = 'none';
  }
  
  // Close modal when clicking outside the image
  modal.onclick = function(event) {
    if (event.target == modal) {
      modal.style.display = 'none';
    }
  }
  
  // Close modal with ESC key
  document.addEventListener('keydown', function(event) {
    if (event.key === 'Escape') {
      modal.style.display = 'none';
    }
  });
});
</script>

<!-- twitter -->
<meta name="twitter:title" content="Whole-Body Conditioned Egocentric Video Prediction">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://bair.berkeley.edu/static/blog/peva/teaserv3_web.png">

<meta name="keywords" content="World Model, Whole-Body World Model, Robotics, Egocentric Video Prediction">
<meta name="description" content="The BAIR Blog">
<meta name="author" content="Yutong Bai, Danny Tran, Amir Bar, Yann LeCun, Trevor Darrell, Jitendra Malik">

<div style="width: 100%; margin: 0 auto; text-align: center;">
<p style="text-align:center;">
<img src="{{ site.baseurl }}/assets/peva/teaserv3_web.png" width="100%">
<br>
<i style="font-size: 0.9em;"><a href="https://arxiv.org/abs/2506.21552" target="_blank"><strong>Predicting Ego-centric Video from human Actions (PEVA)</strong></a>. Given past video frames and an action specifying a desired change in 3D pose, PEVA predicts the next video frame. Our results show that, given the first frame and a sequence of actions, our model can generate videos of atomic actions (a), simulate counterfactuals (b), and support long video generation (c).</i>
</p>
</div>

Recent years have brought significant advances in world models that learn to simulate future outcomes for planning and control. From intuitive physics to multi-step video prediction, these models have grown increasingly powerful and expressive. But few are designed for truly embodied agents. In order to create a World Model for Embodied Agents, we need a <em>real</em> embodied agent that acts in the <em>real</em> world. A <em>real</em> embodied agent has a real physically grounded complex action space as opposed to abstract control signals. They also must act in the <em>real</em> world with diverse real-life scenarios and egocentric view as opposed to aesthetic scenes and stationary cameras.

<!--more-->

<div style="text-align: center; margin: 30px auto;">
<img src="{{ site.baseurl }}/assets/peva/PEVA-summary.png" style="max-width: 70%; height: auto; display: block; margin: 0 auto;" title="Click to enlarge">
</div>

<p style="text-align: center; font-size: 0.85em; color: #666; margin-top: 10px; padding: 8px; background-color: #f5f5f5; border-radius: 4px;"><em>üí° Tip: Click on any image to view it in full resolution.</em></p>

## Why It's Hard

* **Action and vision are heavily context-dependent.** The same view can lead to different movements and vice versa‚Äîbecause humans act in complex, embodied, goal-directed environments.
* **Human control is high-dimensional and structured.** Full-body motion spans 48+ degrees of freedom with hierarchical, time-dependent dynamics‚Äînot synthetic control codes.
* **Egocentric view reveals intention but hides the body.** First-person vision reflects goals, but not motion execution‚Äîmodels must infer consequences from invisible physical actions.
* **Perception lags behind action.** Visual feedback often comes seconds later, requiring long-horizon prediction and temporal reasoning.

To develop a World Model for Embodied Agents, we must ground our approach in agents that meet these criteria. Humans routinely look first and act second‚Äîour eyes lock onto a goal, the brain runs a brief visual ‚Äúsimulation‚Äù of the outcome, and only then does the body move. At every moment, our egocentric view both serves as input from the environment and reflects the intention/goal behind the next movement. When we consider our body movements, we should consider both actions of the feet (locomotion and navigation) and the actions of the hand (manipulation), or more generally, whole-body control.

## What Did We Do?

<p style="text-align: center;">
<img src="{{ site.baseurl }}/assets/peva/what_did_we_do_web.png" width="80%">
</p>
We trained a model to <span style="font-weight:bold;">P</span>redict <span style="font-weight:bold;">E</span>go-centric <span style="font-weight:bold;">V</span>ideo from human <span style="font-weight:bold;">A</span>ctions (<a href="https://arxiv.org/abs/2506.21552" target="_blank">PEVA</a>) for Whole-Body-Conditioned Egocentric Video Prediction. PEVA conditions on kinematic pose trajectories structured by the body's joint hierarchy, learning to simulate how physical human actions shape the environment from a first-person view. We train an autoregressive conditional diffusion transformer on Nymeria, a large-scale dataset pairing real-world egocentric video with body pose capture. Our hierarchical evaluation protocol tests increasingly challenging tasks, providing comprehensive analysis of the model's embodied prediction and control abilities. This work represents an initial attempt to model complex real-world environments and embodied agent behaviors through human-perspective video prediction.

## Method

### Structured Action Representation from Motion
To bridge human motion and egocentric vision, we represent each action as a rich, high-dimensional vector capturing both full-body dynamics and detailed joint movements. Instead of using simplified controls, we encode global translation and relative joint rotations based on the body's kinematic tree. Motion is represented in 3D space with 3 degrees of freedom for root translation and 15 upper-body joints. Using Euler angles for relative joint rotations yields a 48-dimensional action space (3 + 15 √ó 3 = 48). Motion capture data is aligned with video using timestamps, then converted from global coordinates to a pelvis-centered local frame for position and orientation invariance. All positions and rotations are normalized to ensure stable learning. Each action captures inter-frame motion changes, enabling the model to connect physical movement with visual consequences over time.

### Design of PEVA: Autoregressive Conditional Diffusion Transformer

<div style="width: 100%; margin: 0 auto; text-align: center;">
<p style="text-align:center;">
<img src="{{ site.baseurl }}/assets/peva/method_web.png" width="100%">
<br>
</p>
</div>

While the CDiT from Navigation World Models uses simple control signals like velocity and rotation, modeling whole-body human motion presents greater challenges. Human actions are high-dimensional, temporally extended, and physically constrained. To address these challenges, we extend the CDiT method in three ways:

* **Random Timeskips**: Allows the model to learn both short-term motion dynamics and longer-term activity patterns.
* **Sequence-Level Training**: Models entire motion sequences by applying loss over each frame prefix.
* **Action Embeddings**: Concatenates all actions at time t into a 1D tensor to condition each AdaLN layer for high-dimensional whole-body motion.

### Sampling and Rollout Strategy
At test time, we generate future frames by conditioning on a set of past context frames. We encode these frames into latent states and add noise to the target frame, which is then progressively denoised using our diffusion model. To speed up inference, we restrict attention, where within image attention is applied only to the target frame and context cross attention is only applied for the last frame. For action-conditioned prediction, we use an autoregressive rollout strategy. Starting with context frames, we encode them using a VAE encoder and append the current action. The model then predicts the next frame, which is added to the context while dropping the oldest frame, and the process repeats for each action in the sequence. Finally, we decode the predicted latents into pixel-space using a VAE decoder.

### Atomic Actions
We decompose complex human movements into atomic actions‚Äîsuch as hand movements (up, down, left, right) and whole-body movements (forward, rotation)‚Äîto test the model's understanding of how specific joint-level movements affect the egocentric view. We include some samples here:

<div style="width: 90%; margin: 0 auto;">
  
  <!-- Body Movement Actions -->
  <h4 style="text-align: center; margin: 20px 0 10px 0;">Body Movement Actions</h4>
  <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin-bottom: 20px;">
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/atomic_actions_v3/move_forward.png" width="100%">
      <i style="font-size: 0.9em;">Move Forward</i>
    </div>
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/atomic_actions_v3/rotate_left.png" width="100%">
      <i style="font-size: 0.9em;">Rotate Left</i>
    </div>
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/atomic_actions_v3/rotate_right.png" width="100%">
      <i style="font-size: 0.9em;">Rotate Right</i>
    </div>
  </div>
  
  <!-- Left Hand Actions -->
  <h4 style="text-align: center; margin: 20px 0 10px 0;">Left Hand Actions</h4>
  <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin-bottom: 20px;">
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/atomic_actions_v3/move_left_hand_up.png" width="100%">
      <i style="font-size: 0.9em;">Move Left Hand Up</i>
    </div>
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/atomic_actions_v3/move_left_hand_down.png" width="100%">
      <i style="font-size: 0.9em;">Move Left Hand Down</i>
    </div>
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/atomic_actions_v3/move_left_hand_left.png" width="100%">
      <i style="font-size: 0.9em;">Move Left Hand Left</i>
    </div>
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/atomic_actions_v3/move_left_hand_right.png" width="100%">
      <i style="font-size: 0.9em;">Move Left Hand Right</i>
    </div>
  </div>
  
  <!-- Right Hand Actions -->
  <h4 style="text-align: center; margin: 20px 0 10px 0;">Right Hand Actions</h4>
  <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;">
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/atomic_actions_v3/move_right_hand_up.png" width="100%">
      <i style="font-size: 0.9em;">Move Right Hand Up</i>
    </div>
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/atomic_actions_v3/move_right_hand_down.png" width="100%">
      <i style="font-size: 0.9em;">Move Right Hand Down</i>
    </div>
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/atomic_actions_v3/move_right_hand_left.png" width="100%">
      <i style="font-size: 0.9em;">Move Right Hand Left</i>
    </div>
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/atomic_actions_v3/move_right_hand_right.png" width="100%">
      <i style="font-size: 0.9em;">Move Right Hand Right</i>
    </div>
  </div>
  
</div>

### Long Rollout
Here you can see the model's ability to maintain visual and semantic consistency over extended prediction horizons. We demonstrate some samples of PEVA generating coherent 16-second rollouts conditioned on full-body motion. We include some video samples and image samples for closer viewing here:

<div style="width: 90%; margin: 0 auto;">
  <!-- Animated GIF -->
  <div style="text-align: center; margin: 30px 0;">
    <img src="{{ site.baseurl }}/assets/peva/long_seq_v2_compressed.gif" width="100%" style="border-radius: 5px;">
  </div>
  
  <!-- Three sample sequences in a row -->
  <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin-bottom: 30px;">
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/id_34_web.png" width="100%">
      <i style="font-size: 0.85em;">Sequence 1</i>
    </div>
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/id_47_web.png" width="100%">
      <i style="font-size: 0.85em;">Sequence 2</i>
    </div>
    <div style="text-align: center;">
      <img src="{{ site.baseurl }}/assets/peva/id_86_web.png" width="100%">
      <i style="font-size: 0.85em;">Sequence 3</i>
    </div>
  </div>
</div>

### Planning
PEVA can be used to plan a human in-the-loop by simulating multiple action candidates and scoring them based on their perceptual similarity to the goal, as measured by LPIPS.

<div style="width: 75%; margin: 0 auto; text-align: center;">
<p style="text-align:center;">
<img src="{{ site.baseurl }}/assets/peva/counterfactuals_v3_1_web.png" width="100%" title="Click to enlarge">
<br>
<i style="font-size: 0.9em;">In this example, it rules out paths that lead to the sink or outdoors finding the correct path to open the fridge.</i>
</p>
</div>

<div style="width: 75%; margin: 0 auto; text-align: center;">
<p style="text-align:center;">
<img src="{{ site.baseurl }}/assets/peva/counterfactuals_v3_2_web.png" width="100%" title="Click to enlarge">
<br>
<i style="font-size: 0.9em;">In this example, it rules out paths that lead to grabbing nearby plants and going to the kitchen while finding reasonable sequence of actions that lead to the shelf.</i>
</p>
</div>

### Enables Visual Planning Ability
We remove the human in-the-loop to achieve a visual target. We formulate planning as an energy minimization problem and perform standalone planning in the same way as NWM (Bar et al., 2025) using the Cross-Entropy Method (CEM) (Rubinstein, 1997) besides minor modifications in the representation and initialization of the action. We predict sequences of actions over either the left or right arm. We demonstrate some examples here:

<div style="width: 75%; margin: 0 auto; text-align: center;">
<p style="text-align:center;">
<img src="{{ site.baseurl }}/assets/peva/right_id_18.png" width="100%">
<br>
<i style="font-size: 0.9em;">In this case, we are able to predict a sequence of actions that raises our right arm to the mixing stick. We see a limitation with our method as we only predict the right arm so we do not predict to move the left arm down accordingly.</i>
</p>
</div>

<div style="width: 75%; margin: 0 auto; text-align: center;">
<p style="text-align:center;">
<img src="{{ site.baseurl }}/assets/peva/right_kettle.png" width="100%">
<br>
<i style="font-size: 0.9em;">In this case, we are able to predict a sequence of actions that reaches toward the kettle but does not quite grab it as in the goal.</i>
</p>
</div>

<div style="width: 75%; margin: 0 auto; text-align: center;">
<p style="text-align:center;">
<img src="{{ site.baseurl }}/assets/peva/left_id_4.png" width="100%">
<br>
<i style="font-size: 0.9em;">In this case, we are able to predict a sequence of actions that pulls our left arm in, similar to the goal.</i>
</p>
</div>


## Quantitative Results

We evaluate PEVA across multiple metrics to demonstrate its effectiveness in generating high-quality egocentric videos from whole-body actions. Our model consistently outperforms baselines in perceptual quality, maintains coherence over long time horizons, and shows strong scaling properties with model size.

<h3 style="text-align: center;">Baseline Perceptual Metrics</h3>

<div style="width: 85%; margin: 20px auto; text-align: center;">
<img src="{{ site.baseurl }}/assets/peva/baselines.png" width="50%" title="Click to enlarge">
<p style="margin-top: 10px; text-align: center;"><i style="font-size: 0.9em;">Baseline perceptual metrics comparison across different models.</i></p>
</div>

<h3 style="text-align: center;">Atomic Action Performance</h3>

<div style="width: 85%; margin: 20px auto; text-align: center;">
<img src="{{ site.baseurl }}/assets/peva/atomic_action_quantitative.png" width="100%" title="Click to enlarge">
<p style="margin-top: 10px; text-align: center;"><i style="font-size: 0.9em;">Comparison of models in generating videos of atomic actions.</i></p>
</div>

<!-- <h3 style="text-align: center;">Video Quality</h3>

<div style="width: 85%; margin: 20px auto; text-align: center;">
<img src="{{ site.baseurl }}/assets/peva/video_quality.png" width="100%" title="Click to enlarge">
<p style="margin-top: 10px;"><i style="font-size: 0.9em;">Video Quality Across Time (FID).</i></p>
</div> -->

<h3 style="text-align: center;">FID Comparison</h3>

<div style="width: 85%; margin: 20px auto; text-align: center;">
<img src="{{ site.baseurl }}/assets/peva/fid_comparison_web.png" width="100%" title="Click to enlarge">
<p style="margin-top: 10px; text-align: center;"><i style="font-size: 0.9em;">FID comparison across different models and time horizons.</i></p>
</div>

<h3 style="text-align: center;">Scaling</h3>

<div style="width: 85%; margin: 20px auto; text-align: center;">
<img src="{{ site.baseurl }}/assets/peva/scaling.png" width="80%" title="Click to enlarge">
<p style="margin-top: 10px; text-align: center;"><i style="font-size: 0.9em;">PEVA has good scaling ability. Larger models lead to better performance.</i></p>
</div>

## Future Directions
Our model demonstrates promising results in predicting egocentric video from whole-body motion, but it remains an early step toward embodied planning. Planning is limited to simulating candidate arm actions and lacks long-horizon planning and full trajectory optimization. Extending PEVA to closed-loop control or interactive environments is a key next step. The model currently lacks explicit conditioning on task intent or semantic goals. Our evaluation uses image similarity as a proxy objective. Future work could leverage combining PEVA with high-level goal conditioning and integrating object-centric representations.


## Acknowledgements
The authors thank Rithwik Nukala for his help in annotating atomic actions. We thank <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a>, <a href="https://www.cs.utexas.edu/~philkr/">Philipp Kr√§henb√ºhl</a>, <a href="https://www.cs.cornell.edu/~bharathh/">Bharath Hariharan</a>, <a href="https://guanyashi.github.io/">Guanya Shi</a>, <a href="https://shubhtuls.github.io/">Shubham Tulsiani</a> and <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a> for the useful suggestions and feedbacks for improving the paper; <a href="https://www.cis.upenn.edu/~jshi/">Jianbo Shi</a> for the discussion regarding control theory; <a href="https://yilundu.github.io/">Yilun Du</a> for the support on Diffusion Forcing; <a href="https://brentyi.com/">Brent Yi</a> for his help in human motion related works and <a href="https://people.eecs.berkeley.edu/~efros/">Alexei Efros</a> for the discussion and debates regarding world models. This work is partially supported by the ONR MURI N00014-21-1-2801.

<hr>

<p style="text-align: center;">
<strong>For more details, read the <a href="https://arxiv.org/abs/2506.21552" target="_blank">full paper</a> or visit the <a href="https://dannytran123.github.io/PEVA/" target="_blank">project website</a>.</strong>
</p>