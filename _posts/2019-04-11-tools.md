---
layout:             post
title:              "Robots that Learn to Use Improvised Tools"
date:               2019-04-11 9:00:00
author:             Annie Xie
img:                assets/tools/chimp.jpg
excerpt_separator:  <!--more-->
visible:            True
show_comments:      False
---

In many animals, tool-use skills emerge from a combination of observational
learning and experimentation. For example, by watching one another, chimpanzees
can learn how to use twigs to “fish” for insects. Similarly, [capuchin
monkeys][1] demonstrate the ability to wield sticks as sweeping tools to pull
food closer to themselves. While one might wonder whether these are just
illustrations of “monkey see, monkey do,” we believe these tool-use abilities
indicate a greater level of intelligence.

<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/chimp.jpg"
    height="280" style="margin: 10px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/gorilla.jpg"
    height="280" style="margin: 10px;">
    <br>
<i>
Left: A chimpanzee fishing for termites. Right: A gorilla using a stick to
gather herbs. (<a href="https://en.wikipedia.org/wiki/Tool_use_by_animals#Hunting">source</a>)
</i>
</p>

The question our new work explores is: can we enable robots to use tools in the
same way --- through observation and experimentation?

A requisite for performing complex multi-object manipulation tasks, such as
those involved in tool use, is an understanding of physical cause-and-effect
relationships. Therefore, the ability to *predict* how one object might
interact with another is crucial. Our [prior work][2] has investigated how
visual predictive models of cause-and-effect can be learned from unsupervised
robot interaction with the world. After learning such a model, the robot can
plan to accomplish a diverse set of simple tasks, including cloth folding and
object arrangement.  However, if we consider the more complex interactions that
occur in tool-use tasks, such as how a broom can sweep dirt into a dustpan,
undirected experimentation isn't enough.

Hence, taking inspiration from how animals learn, we designed an algorithm that
allows robots to learn tool-use skills through a similar paradigm of imitation
and interaction. In particular, we show that, with a mix of demonstration data
and unsupervised experience, a robot can use novel objects as tools and even
*improvise* tools in the absence of traditional ones. Further, depending on the
demands of the task, our method demonstrates the ability to *decide* whether to
use the provided tools. In this post, we will describe how this works.


<!--more-->

<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/teaser_0_task.png"
    height="200" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/teaser_1_task.png"
    height="200" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/teaser_2_task.png"
    height="200" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/teaser_0.gif"
    height="200" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/teaser_1.gif"
    height="200" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/teaser_2.gif"
    height="200" style="margin: 1px;">
    <br>
<i>
Our approach enables the robot to figure out how to use diverse objects as
tools to achieve user-specified goals (marked by the yellow arrows). The robot
wasn't told to use the provided tools but determined that it should from the
task.
</i>
</p>


# Guided Visual Foresight

## Learning from Demonstrations

First, we will use a dataset of demonstrations that illustrates how various
tools can be used. Because we ultimately hope to learn a model that is useful
for a diverse range of tool-use skills, we collect demonstrations for a variety
of tasks with a variety of tools. For each demonstration, we record the
sequence of images from the robot’s camera, gripper positions, and commanded
actions.

<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/demo_0.gif"
    height="150" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/demo_1.gif"
    height="150" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/demo_2.gif"
    height="150" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/demo_3.gif"
    height="150" style="margin: 1px;">
    <br>
<i>
Examples of kinesthetic demonstrations.
</i>
</p>

With this data, we can fit a model that proposes sequences of actions that
enable the robot to use objects in the current scene as tools. And, to capture
the range of behaviors in the demonstrations, the action proposal model outputs
a distribution over action sequences.

## Unsupervised Data Collection for a Visual Predictive Model

Since we want the robot to go beyond the behaviors in the demonstrations, and
to generalize to new objects and new situations, we need a lot of diverse data.
That is, data that can be collected in a scalable way by the robot itself. For
example, we want the robot to understand how small mistakes, such as slightly
imperfect grasping, might affect the future. So, we allow the robot to expand
upon its experiences by collecting data on its own.

In particular, the robot autonomously collects data in two different ways: by
taking random sequences of actions and by sampling from the action proposal
model introduced in the previous section. The latter allows the robot to grasp
at tools and move them randomly. This experience is crucial to learn about
multi-object interactions.

<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/data_0.gif"
    height="120" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/data_1.gif"
    height="120" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/data_2.gif"
    height="120" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/data_3.gif"
    height="120" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/data_4.gif"
    height="120" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/data_5.gif"
    height="120" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/data_6.gif"
    height="120" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/data_7.gif"
    height="120" style="margin: 1px;">
    <br>
<i>
Unsupervised interactions with household objects and tools.
</i>
</p>

Our final dataset is composed of the expert demonstrations, the robot’s
unsupervised experiences with various tools, and data from the BAIR Robot
Interaction Dataset. We use this dataset to train a dynamics model. Implemented
as a recurrent convolutional neural network, the model takes as input the
previous image and an action at each timestep and generates the next image.

## Demonstration-Guided Planning

At test time, the robot can now use the model trained with imitation to guide
the planning process and the predictive model to determine which actions will
allow it to perform the task at hand.

New tasks are specified through user-provided key clicks. For example, we can
ask the robot to move the pile of trash onto the dustpan by selecting the
center points of the rubbish and the desired final positions (see below).
Specifying the task in this way does not tell the robot how to use the tool or
even which tool to use in scenarios with multiple candidates, and the robot has
to figure this out during the planning process.

<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/dustpan_task.png"
    height="300">
</p>

We use a sampling-based planning procedure that leverages the action proposal
and video prediction models and allows the robot to accomplish a variety of
tasks with a number of different tools and objects. In particular, action
sequences are initially sampled at random and from the action proposal model.
Then, using the video prediction model, we predict the outcome of each plan.


<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/video_pred_0.gif"
    height="062" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/video_pred_1.gif"
    height="062" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/video_pred_2.gif"
    height="062" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/video_pred_3.gif"
    height="062" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/video_pred_4.gif"
    height="062" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/video_pred_5.gif"
    height="062" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/video_pred_6.gif"
    height="062" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/video_pred_7.gif"
    height="062" style="margin: 1px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/video_pred_8.gif"
    height="062" style="margin: 1px;">
    <br>
<i>
Video predictions corresponding to different action sequences for the same
initial scene.
</i>
</p>

By taking the top plans and fitting a distribution to them, we can repeatedly
sample and improve upon the best one, which is then executed on the robot.

<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/method_diagram.gif"
    height="" style="">
</p>




# Experiments

We experiment with this approach to enable robots to use new tools and achieve
user-specified goals.

<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/sweeper_task.png"
    height="200" style="margin: 2px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/sweeper_pred.gif"
    height="200" style="margin: 2px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/sweeper_exec.gif"
    height="200" style="margin: 2px;">
    <br>
<i>
Left: Initial scene with arrows indicating specified task. Middle: Video
prediction corresponding to the best plan. Right: Execution of the plan on the
robot.
</i>
</p>

In the task shown earlier, the robot uses the nearby sweeper to perform the
task more efficiently:

<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/dustpan_task.png"
    height="280" style="margin: 2px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/dustpan.gif"
    height="280" style="margin: 2px;">
</p>

Even though the robot has never seen a sponge before, it can figure out how to
use it to clean debris off a plate:

<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/teaser_1_task.png"
    height="280" style="margin: 2px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/teaser_1.gif"
    height="280" style="margin: 2px;">
</p>

In the following example, the robot is only allowed to move within the shaded
green region and tasked with moving the blue cylinder towards itself.
Critically, the robot figures out how to use the L-shaped hook to accomplish
the task:

<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/hook_task.jpg"
    height="280" style="margin: 2px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/hook.gif"
    height="280" style="margin: 2px;">
</p>

And, even when presented with an ordinary object such as a bottle, the robot
can infer how to use it as a tool for the task:

<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/bottle_task.png"
    height="280" style="margin: 2px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/bottle.gif"
    height="280" style="margin: 2px;">
</p>

Finally, in situations where it’s better to not use a tool, the robot chooses
to complete the task with its own gripper:

<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/yes_tool_task.png"
    height="240" style="margin: 2px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/yes_tool.gif"
    height="240" style="margin: 2px;">
<br>
<i>
Scenario 1: The robot uses the tool to more efficiently move the two objects.
</i>
</p>

<p style="text-align:center;">
    <img src="http://bair.berkeley.edu/static/blog/tools/no_tool_task.png"
    height="240" style="margin: 2px;">
    <img src="http://bair.berkeley.edu/static/blog/tools/no_tool.gif"
    height="240" style="margin: 2px;">
<br>
<i>
Scenario 2: The robot ignores the hook and moves the single object with its own
gripper.
</i>
</p>


Beyond these examples, our quantitative results in the paper suggest that our
approach is *more general* than learning from demonstrations and *more capable*
than learning from experience alone.

# Related Work on Robotic Tool-Use

[Prior][3] [works][4] have studied tool manipulation with logic programming and
known models under the task and motion planning framework. However, logic-based
and analytic model-based systems are susceptible to modelling errors, which can
accumulate during test-time execution.

Other works have decomposed tool use into [task][5]-[oriented][6] [grasping][7]
of the tool and using the tool with [planning][8] or [policy learning][9].
These methods constrain the scope of motions to those that involve the tool,
while our approach is capable of finding plans with or without the tool based
on the situation.

[Some][10] [approaches][11] have also proposed learning dynamics models for
tool use.  However, in contrast to these methods, which either use
hand-designed perception pipelines or forgo perception entirely, our approach
learns about object interactions directly from raw image pixels.

# Conclusion

Performing tasks that are both *diverse* and *complex* involving
previously-unseen objects is a challenging undertaking in robotics. To study
this problem, we focused on a variety of tasks that require manipulation of
objects as tools. We demonstrated how our approach, which combines imitation
and self-supervised interaction, can enable robots to accomplish complex
multi-object tasks with a multitude of objects, and even use improvised tools
under new scenarios. We hope that this work represents a step towards making
robots simultaneously more *general* and more *capable*, so that they one day
can perform useful tasks in everyday environments.

<hr>

This post is based on the following paper:

- **[Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight][12]**<br>
  Annie Xie, Frederik Ebert, Sergey Levine, Chelsea Finn

I would like to thank Chelsea Finn and Sergey Levine for their valuable feedback when preparing this blog post.


[1]:https://en.wikipedia.org/wiki/Tool_use_by_animals#Monkeys
[2]:https://bair.berkeley.edu/blog/2018/11/30/visual-rl/
[3]:http://www.roboticsproceedings.org/rss14/p44.pdf
[4]:https://link.springer.com/chapter/10.1007/978-3-642-38812-5_1
[5]:https://ieeexplore.ieee.org/document/769
[6]:https://ieeexplore.ieee.org/document/6630999
[7]:https://arxiv.org/abs/1810.04438
[8]:https://cs.stanford.edu/people/asaxena/papers/deepmpc_rss2015.pdf
[9]:https://arxiv.org/abs/1806.09266
[10]:https://ieeexplore.ieee.org/document/1570580
[11]:https://cs.stanford.edu/people/asaxena/papers/deepmpc_rss2015.pdf
[12]:https://drive.google.com/file/d/1x0iNRGtcdVhgL5D1UNjDoR4VT5VSzcoW/view
