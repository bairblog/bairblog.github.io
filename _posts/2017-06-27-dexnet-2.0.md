---
layout:     post
title:      "Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics."
date:       2017-06-27 10:00:00
author:     Jeff Mahler
visible:    True
excerpt_separator: <!--more-->
---

[![IMAGE ALT TEXT](http://i.imgur.com/BwJ88wo.jpg)](https://www.youtube.com/watch?v=i6K3GI2_EgU "Dex-Net 2.0: 99% Precision Grasping")

*This blog post summarizes new results that will be presented at the Robotics:
Science and Systems Conference at MIT in July 2017 (1).*

Reliable robotic grasping across many objects is challenging due to sensor
noise, which makes it difficult for a robot to precisely infer physical
properties such as object shape, pose, material properties, mass, and the
locations of contact points between the fingers and object. Recent results
suggest that deep neural networks trained on large datasets of human grasp
labels (2) or trials of grasping on a physical system (3) can be used to plan
successful grasps across a wide variety of objects directly from images (4) with
no explicit modeling of physics, similar to generalization results seen in
computer vision. However, current methods require months of execution time on a
physical system or tedious human hand-labeling of millions of examples to learn
to grasp with up to 90% success.

An alternative is to use Cloud Computing to rapidly compute grasps across a
large dataset of object mesh models (5) using physics-based models of grasping
(6). These methods rank potential grasps according to success metrics, such as
whether or not the grasp can resist arbitrary forces and torques, evaluated
under perturbations in properties such as object pose and friction coefficient
to model the effects of sensor noise (7). However, they make the strong
assumption of a perception system that estimates these properties either
perfectly or according to known Gaussian distributions. In practice, these
perception systems are slow, prone to errors, and may not generalize well to new
objects. Thus, despite over 30 years of research, in practice it is common to
plan grasps using heuristics such as cylinder detection in applications such as
home decluttering (8) and the Amazon Picking Challenge (9).

<!--more-->

# Dex-Net 2.0: A Hybrid Approach

We propose an alternative approach that attempts to leverage the best of both
methods: to use deep Convolutional Neural Networks (CNNs) to generalize grasps
across a wide variety of objects and physics-based models to quickly generate
the massive datasets of images and grasps needed to reliably train them.

Rather than attempt to precisely infer quantities such as 3D object shape and
pose from images, we propose to construct a probabilistic model that can
generate synthetic point clouds, grasps, and grasp success metrics from datasets
of 3D object meshes (10) using physical models of grasping, image rendering, and
camera noise. Our key insight is that possible correlations between grasp
success and geometric features in point clouds, such as handles and cylinders,
will be evident in samples from the model, and Deep CNNs may be able to leverage
these correlations using a hierarchical set of filters similar to the Gabor-like
filters learned by CNNs for image classification (11).

We formalize and study this approach in our paper, ["Dex-Net 2.0: Deep Learning
to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp
Metrics."](https://arxiv.org/abs/1703.09312) In the paper we introduce the
Dexterity Network (Dex-Net) 2.0, a dataset of 6.7 million robust grasps and
point clouds with synthetic noise generated from our probabilistic model of
grasping rigid objects on a tabletop with a parallel-jaw gripper. We develop a
deep Grasp Quality Convolutional Neural Network (GQ-CNN) model and train it on
Dex-Net 2.0 to estimate the probability of success (or *robustness*) of grasps
directly from point clouds. Then we use the GQ-CNN to plan grasps on a physical
robot by sampling a set of grasp candidates from an input point cloud with edge
detection and executing the most robust grasp estimated by the GQ-CNN:

![alt text](http://i.imgur.com/ChTDZKW.png "Dex-Net 2.0 Grasping Policy")

When trained on Dex-Net 2.0, the GQ-CNN learns a set of low-level filters that
appear to detect image gradients at various scales. Filters can be organized
into two classes: coarse oriented gradient filters that may be useful for
estimating collisions between the gripper and object and fine vertical filters
that may be useful for estimating surface normals at the locations of contact
between the fingers and object:

![alt text](http://i.imgur.com/OelLCuA.png "Grasp Quality Convolutional Neural Network")

# Experiments with the ABB YuMi

To evaluate the Dex-Net 2.0 grasp planner on a physical robot, we ran over 1,000
trials of grasping on an [ABB YuMi][1] to investigate:

1. **Model Performance:** Can a GQ-CNN trained entirely on synthetic data for a
set of known objects be used to successfully grasp the objects on a physical
robot?
2. **Generalization:** Can the GQ-CNN be used to successfully grasp novel
objects that were not seen in training?

### Model Performance

We first measured the ability of our method to plan grasps that could maintain a
grasp on the object while lifting the object, transporting it, and shaking it
within the gripper. We used a set of eight 3D printed objects with known
shape, center of mass, and frictional properties to highlight differences
between our physical models and grasping on the physical robot. In particular we
chose objects with *adversarial* geometry for two-finger grippers such as smooth,
curved surfaces and narrow openings to explore failure modes.

We found that the Dex-Net 2.0 grasp planner could achieved up to 93% success on
the physical robot and was 3x faster than a method that matched the exact object
shape to the point cloud. The results suggest that our physics-based model is a
useful approximation of reality for grasp planning when object properties are
known and that the GQ-CNN can be used to plan highly precise grasps. Here's an
example: 

<p style="text-align:center;"> 
<img src="{{site.url}}/assets/dexnet2/dex-net.gif"> 
</p>

### Generalization

We also evaluated the ability to generalize to previously unseen objects by
testing on a set of 40 novel objects including objects with moving parts, such
as a can opener, and deformability, such as a washcloth. After analyzing the
data further we found a surprising result: the GQ-CNN predicted zero false
positives out of 69 grasps predicted to succeed. This 99% precision score
suggests that the robot could achieve high reliability by asking a human for
help or poking objects when it is not confident that a grasp will succeed.

# Limitations

The results of grasp planning with Dex-Net 2.0 suggest that it is possible to
achieve highly reliable grasping across a wide variety of objects by training
neural networks with only synthetic data generated using physical models of
grasping and image formation. However, there are several limitations of the
current method:

1. **Sensor Capabilities.** There are some sources of noise on the physical
depth camera, such as missing data, that are not accounted for by the Dex-Net
2.0 model. Furthermore, depth cameras cannot see objects that are transparent or
flat on a table.
2. **Model Limitations.** The physical model of grasping used by Dex-Net 2.0
considers fingertip grasps of rigid objects. We do not account for grasping
strategies such as pinching a flat piece of paper into the gripper or hooking an
object with a finger.
3. **Single Objects.** The method is designed to only grasp objects in
isolation. We are currently working on extending the Dex-Net 2.0 model to
grasping objects from a pile.
4. **Task-Independence.** The method plans grasps that can be used to robustly
lift and transport an object but does not consider use cases of an object such
as exact placement, stacking, or connecting it to another object in assembly
which require more precise grasps. We are researching possible extensions with
task-based grasp quality metrics, dynamic simulation, and learning from
demonstration.

# Dataset and Code Release

To make progress toward overcoming these limitations, we plan to release a
subset of our code, datasets, and the trained GQ-CNN weights over the summer
which we hope will facilitate further research and comparisons.

We are aiming for the following releases and dates:
* **GQ-CNN Package:** Dex-Net 2.0 GQ-CNN training dataset with 6.7 million datapoints and ROS integration. *June 19, 2017.*
* **Read Only Dex-Net 2.0:** Dex-Net 2.0 Database with Python API that can be used to generate GQ-CNN training datasets for new cameras and grippers. *July 11, 2017.*
* **Full Dex-Net 2.0:** Dex-Net 2.0 Python API to create new databases with custom 3D models and custom grasp success metricss and a Web API to upload new models: *September 1, 2017.*

We also plan to keep a leaderboard of performance on the Dex-Net 2.0 dataset to
investigate improvements to the GQ-CNN architecture, since our best models
achieve only 91% classification accuracy on synthetic data. We will also
evaluate the performance on the physical robot for models that signficantly
outperform other methods on synthetic data. We invite researchers from any
discipline or background to participate.
 
### Contact
See the [project website][2] for updates and progress.

For more information please contact [Jeff Mahler][3] or [Prof. Ken Goldberg][4]
of [the Berkeley AUTOLAB][3].

### Acknowledgments

This research was performed at the [AUTOLAB](http://autolab.berkeley.edu/) at UC
Berkeley in affiliation with the Berkeley AI Research (BAIR) Lab, the Real-Time
Intelligent Secure Execution (RISE) Lab, and the CITRIS People and Robots (CPAR)
Initiative. The authors were supported in part by the U.S. National Science
Foundation under NRI Award IIS-1227536: Multilateral Manipulation by Human-Robot
Collaborative Systems, the Department of Defense (DoD) through the National
Defense Science & Engineering Graduate Fellowship (NDSEG) Program, the Berkeley
Deep Drive (BDD) Program, and by donations from Siemens, Google, Cisco,
Autodesk, IBM, Amazon Robotics, and Toyota Robotics Institute. Any opinions,
findings, and conclusions or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of the Sponsors.

### References

(1): Mahler, Jeffrey, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, and Ken Goldberg. "Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics." arXiv preprint arXiv:1703.09312 (2017). [(Paper)](https://arxiv.org/abs/1703.09312) [(Website)](http://berkeleyautomation.github.io/dex-net/)

(2): Kappler, Daniel, Jeannette Bohg, and Stefan Schaal. "Leveraging Big Data for Grasp Planning." In Robotics and Automation (ICRA), 2015 IEEE International Conference on, pp. 4304-4311. IEEE, 2015.

(3): Levine, Sergey, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection." arXiv preprint arXiv:1603.02199 (2016).

(4): Johns, Edward, Stefan Leutenegger, and Andrew J. Davison. "Deep Learning a Grasp Function for Grasping under Gripper Pose Uncertainty." In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on, pp. 4461-4468. IEEE, 2016.

(5): Goldfeder, Corey, Matei Ciocarlie, Hao Dang, and Peter K. Allen. "The Columbia Grasp Database." In Robotics and Automation, 2009. ICRA'09. IEEE International Conference on, pp. 1710-1716. IEEE, 2009.

(6): Prattichizzo, Domenico, and Jeffrey C. Trinkle. "Grasping." In Springer Handbook of Robotics, pp. 955-988. Springer International Publishing, 2016.

(7): Weisz, Jonathan, and Peter K. Allen. "Pose Error Robust Grasping from Contact Wrench Space Metrics." In Robotics and Automation (ICRA), 2012 IEEE International Conference on, pp. 557-562. IEEE, 2012.

(8): Ciocarlie, Matei, Kaijen Hsiao, Edward Gil Jones, Sachin Chitta, Radu Bogdan Rusu, and Ioan A. Şucan. "Towards Reliable Grasping and Manipulation in Household Environments." In Experimental Robotics, pp. 241-252. Springer Berlin Heidelberg, 2014.

(9): Hernandez, Carlos, Mukunda Bharatheesha, Wilson Ko, Hans Gaiser, Jethro Tan, Kanter van Deurzen, Maarten de Vries et al. "Team Delft's Robot Winner of the Amazon Picking Challenge 2016." arXiv preprint arXiv:1610.05514 (2016).

(10): Mahler, Jeffrey, Florian T. Pokorny, Brian Hou, Melrose Roderick, Michael Laskey, Mathieu Aubry, Kai Kohlhoff, Torsten Kröger, James Kuffner, and Ken Goldberg. "Dex-Net 1.0: A Cloud-Based Network of 3D Objects for Robust Grasp Planning using a Multi-Armed Bandit Model with Correlated Rewards." In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pp. 1957-1964. IEEE, 2016.

(11): Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet Classification with Deep Convolutional Neural Networks." In Advances in Neural Information Processing Systems, pp. 1097-1105. 2012.

[1]:http://new.abb.com/products/robotics/industrial-robots/yumi
[2]:berkeleyautomation.github.io/dex-net
[3]:http://www.jeff-mahler.com
[4]:http://goldberg.berkeley.edu/
[5]:http://coeautomation.wpengine.com/
