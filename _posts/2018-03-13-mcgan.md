---
layout:             post
title:              "Transfer Your Font Style with GANs"
date:               2018-03-13 9:00:00
author:             <a href="https://people.eecs.berkeley.edu/~sazadi/">Samaneh Azadi</a>
img:                /assets/mcgan/given-new-titles.jpg
excerpt_separator:  <!--more-->
visible:            True
show_comments:      True
---

<!-- Daniel Seita: I added this to handle Samaneh's post. -->
<script  type="text/javascript">
function update_im_font1(option_font) {
    document.getElementById("im_font1").src="http://bair.berkeley.edu/static/blog/mcgan/"+option_font+".png";
}
function update_im_font2(option_font) {
    document.getElementById("im_font2").src="http://bair.berkeley.edu/static/blog/mcgan/"+option_font+".png";
}
function update_im_font3(option_font) {
    document.getElementById("im_font3").src="http://bair.berkeley.edu/static/blog/mcgan/"+option_font+".png";
}
function update_im_font4(option_font) {
    document.getElementById("im_font4").src="http://bair.berkeley.edu/static/blog/mcgan/"+option_font+".png";
}
</script>

<p style="text-align:center;">
<img height = "300" src="http://bair.berkeley.edu/static/blog/mcgan/given-new-titles.jpg">
<br>
<i>
Left: Given movie poster, Right: New movie title generated by MC-GAN.
</i>
</p>


Text is a prominent visual element of 2D design. Artists invest significant time
into designing glyphs that are visually compatible with other elements in their
shape and texture. This process is labor intensive and artists often design only
the subset of glyphs that are necessary for a title or an annotation, which
makes it difficult to alter the text after the design is created, or to transfer
an observed instance of a font to your own project.

Early research on glyph synthesis focused on geometric modeling of outlines,
which is limited to particular glyph topology (e.g., cannot be applied to
decorative or hand-written glyphs) and cannot be used with image input.
With the rise of deep neural networks, researchers have looked at modeling
glyphs from images. On the other hand, synthesizing data consistent with 
partial observations is an interesting problem in computer vision and graphics
such as multi-view image generation, completing missing regions in images, 
and generating 3D shapes. Font data is an example that provides a clean factorization
of style and content.

Recent advances in conditional generative adversarial networks (cGANS) [1] have
been successful in many generative applications. However, they do best only with
fairly specialized domains and not with general or multi-domain style transfer.
Similarly, when directly used to generate fonts, cGAN models produce significant
artifacts. For instance, given the following five letters,

<p style="text-align:center;">
<img src="http://bair.berkeley.edu/static/blog/mcgan/given_tower.png" width="20%"><br>
</p>

a conditional GAN model is not successful in generating all 26 letters with the same style:

<p style="text-align:center;">
<img src="http://bair.berkeley.edu/static/blog/mcgan/pix2pix_tower.png" width="100%" /><br />
</p>

<!--more-->

# Multi-Content GAN for Few Shot Font Style Transfer

Instead of training a single network for all possible typeface ornamentations,
we designed the multi-content GAN architecture [2] to retrain a customized
magical network for each observed character set with only a handful of observed
glyphs.  This model considers content (i.e., A-Z glyphs) along channels and
style (i.e., glyph ornamentations) along network layers to transfer the style of
given glyphs to the contents of unseen ones.

The multi-content GAN model consists of a stacked cGAN architecture to predict
the coarse glyph shapes and an ornamentation network to predict color and
texture of the final glyphs. The first network, called GlyphNet, predicts glyph
masks while the second network, called OrnaNet, fine-tunes color and
ornamentation of the generated glyphs from the first network. Each sub-network
follows the conditional generative adversarial network (cGAN) architecture
modified for its specific purpose of stylizing glyphs or ornamentation
prediction.


## Network Architecture

Here is the schematic of GlyphNet to learn the general shape of the font
manifold from a set of training fonts. Input and output of the GlyphNet are
stacks of the glyphs where a channel is assigned for each letter.  In each
training iteration, $x_1$ includes a randomly chosen subset of $y_1$ glyphs with
the remaining input channels being zeroed out.

<p style="text-align:center;">
<img src="http://bair.berkeley.edu/static/blog/mcgan/GlyphNet-simple.jpg" width="90%" /><br />
</p>

With this novel glyph stack design, correlations between different glyphs are
learned across network channels in order to transfer their style automatically.
The following plots represent such correlations through the structural
similarity (SSIM) metric on a random set of 1500 font examples. Computing the
structural similarity between each generated glyph and its ground truth, 25
distributions are found when a single letter has been observed at a time. These
plots show the distributions $\alpha| \beta$ of generating letter $\alpha$ when
letter $\beta$ is observed (in blue) vs when any other letter rather than
$\beta$ is given (in red). Distributions for the two most informative given
letters and the two least informative ones in generating each of the 26 letters
are shown in this figure. For instance, looking at the fifth row of the figure,
letters F and B are the most constructive in generating letter E compared with
other letters while I and W are the least informative ones. As other examples, O
and C are the most guiding letters for constructing G as well as R and B for
generating P.

<p style="text-align:center;">
<img src="http://bair.berkeley.edu/static/blog/mcgan/corr_.png" width="80%" /><br />
</p>

Therefore, for any desirable font with only a few observed letters, the
pre-trained GlyphNet generates all 26 A-Z glyphs. But how should we transfer
ornamentation? The second network, OrnaNet, takes these generated glyphs and
after a simple reshape transformation and gray-scale channel repetition,
represented by $\mathcal{T}$ in the next figure, generates outputs enriched with
desirable color and ornamentation using a conditional GAN architecture. Inputs
and outputs of the OrnaNet are batches of RGB images instead of stacks where the
RGB channels for each letter, as an image, are repeats of its corresponding
gray-scale glyph generated by the GlyphNet. Multiple regularizers in the OrnaNet
penalize deviation of the masks of stylized letters from their corresponding
glyph shapes.

<p style="text-align:center;">
<img src="http://bair.berkeley.edu/static/blog/mcgan/End-to-End-Simple.jpg" width="100%" /><br />
</p>

## Results

Below, we demonstrate exemplar sentences using the font style given in a single
word.


<form >
<p style="text-align:center;">
<font size="5"> Given: </font>
<input type="radio" name="option_font1" onclick="update_im_font1(this.value)" value="generate_ft51_1">
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_generate_ft51_1.png" width="14%">
<input type="radio" name="option_font1" onclick="update_im_font1(this.value)" value="content_ft51_1">
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_content_ft51_1.png" width="14%">
<input type="radio" name="option_font1" onclick="update_im_font1(this.value)" value="transfer_ft51_1" checked>
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_transfer_ft51_1.png" width="14%">
<input type="radio" name="option_font1" onclick="update_im_font1(this.value)" value="texture_ft51_1">
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_texture_ft51_1.png" width="14%">
</p>
</form>
<p style="text-align:center;">
<img src="http://bair.berkeley.edu/static/blog/mcgan/transfer_ft51_1.png" name="im_font1" id="im_font1" width="90%">
</p><br>


<form >
<p style="text-align:center;">
<font size="5"> Given: </font>
<input type="radio" name="option_font2" onclick="update_im_font2(this.value)" value="generate_ft37_1">
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_generate_ft37_1.png" width="14%">
<input type="radio" name="option_font2" onclick="update_im_font2(this.value)" value="content_ft37_1">
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_content_ft37_1.png" width="14%">
<input type="radio" name="option_font2" onclick="update_im_font2(this.value)" value="transfer_ft37_1" checked>
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_transfer_ft37_1.png" width="14%">
<input type="radio" name="option_font2" onclick="update_im_font2(this.value)" value="texture_ft37_1">
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_texture_ft37_1.png" width="14%">
</p>
</form>
<p style="text-align:center;">
<img src="http://bair.berkeley.edu/static/blog/mcgan/transfer_ft37_1.png" name="im_font2" id="im_font2" width="90%">
</p><br>


<form >
<p style="text-align:center;">
<font size="5"> Given: </font>
<input type="radio" name="option_font3" onclick="update_im_font3(this.value)" value="generate_ft55_1">
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_generate_ft55_1.png" width="13%">
<input type="radio" name="option_font3" onclick="update_im_font3(this.value)" value="content_ft55_1">
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_content_ft55_1.png" width="13%">
<input type="radio" name="option_font3" onclick="update_im_font3(this.value)" value="transfer_ft55_1" checked>
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_transfer_ft55_1.png" width="13%">
<input type="radio" name="option_font3" onclick="update_im_font3(this.value)" value="texture_ft55_1">
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_texture_ft55_1.png" width="13%">
</p>
</form>
<p style="text-align:center;">
<img src="http://bair.berkeley.edu/static/blog/mcgan/transfer_ft55_1.png" name="im_font3" id="im_font3" width="90%">
</p><br>



<form >
<p style="text-align:center;">
<font size="5"> Given: </font>
<input type="radio" name="option_font4" onclick="update_im_font4(this.value)" value="generate_SHREK">
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_generate_SHREK.png" width="14.5%">
<input type="radio" name="option_font4" onclick="update_im_font4(this.value)" value="content_SHREK">
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_content_SHREK.png" width="14.5%"> 
<input type="radio" name="option_font4" onclick="update_im_font4(this.value)" value="transfer_SHREK" checked>
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_transfer_SHREK.png" width="14.5%">
<input type="radio" name="option_font4" onclick="update_im_font4(this.value)" value="texture_SHREK">
<img src="http://bair.berkeley.edu/static/blog/mcgan/input_texture_SHREK.png" width="14.5%">
</p>
</form>
<p style="text-align:center;">
<img src="http://bair.berkeley.edu/static/blog/mcgan/transfer_SHREK.png" name="im_font4" id="im_font4" width="90%">
</p><br>

Also, here is the gradual improvement in the OrnaNet prediction:

<p style="text-align:center;">
<img src="http://bair.berkeley.edu/static/blog/mcgan/ft51_1_fake_B.gif" width="100%" /><br />
</p>


# References

<p> <font size="3"> [1] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. "Image-to-Image Translation with Conditional Adversarial Networks." CVPR 2017.</font></p>

<p> <font size="3"> [2] Samaneh Azadi, Matthew Fisher, Vladimir Kim, Zhaowen Wang, Eli Shechtman, and Trevor Darrell. "Multi-Content GAN for Few-Shot Font Style Transfer." CVPR 2018.</font> </p>

# More Information

For more information about Multi-Content GAN, please take a look at the following links:

<ul>
  <li><a href="https://arxiv.org/abs/1712.00516">Arxiv preprint</a></li>
  <li><a href="https://github.com/azadis/MC-GAN">MC-GAN code</a></li>
</ul>

Please let us know if you have any questions or suggestions.
