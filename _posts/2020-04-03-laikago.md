---
layout:             post
title:              "Robots Learning to Move like Animals"
date:               2020-04-03 9:00:00
author:             <a href="https://xbpeng.github.io/">Xue Bin (Jason) Peng</a>
img:                assets/laikago/02_keypoints_robot.png
excerpt_separator:  <!--more-->
visible:            True
show_comments:      False
---

<!--
TODO TODO TODO
Be careful that these three lines are at the top, and that the title and image change for each blog post!

00_teaser.gif
01_overview.gif
02_keypoints_dog.png
02_keypoints_robot.png
03_retarget_pace.gif
04_retarget_spin.gif
05_sim_pace.gif
06_sim_spin.gif
07_policy.png
08_adaptation_pace.gif
09_adaptation_spin.gif
10_real_pace.gif
11_real_trot.gif
12_real_spin.gif
13_real_backward_trot.gif
14_real_sidesteps.gif
14_real_turn.gif
15_real_hopturn.gif
16_comp_trot.gif
-->
<meta name="twitter:title" content="">
<meta name="twitter:card" content="summary_image">
<meta name="twitter:image" content="https://bair.berkeley.edu/static/blog/laikago/02_keypoints_robot.png">


<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/laikago/00_teaser.gif" width=""><br>
<i>
Quadruped robot learning locomotion skills by imitating a dog.
</i>
</p>

Whether it’s a dog chasing after a ball, or a monkey swinging through the
trees, animals can effortlessly perform an incredibly rich repertoire of agile
locomotion skills. But designing controllers that enable legged robots to
replicate these agile behaviors can be a very challenging task. The superior
agility seen in animals, as compared to robots, might lead one to wonder: can
we create more agile robotic controllers with less effort by directly imitating
animals?

In this work, we present a framework for learning robotic locomotion skills by
imitating animals. Given a reference motion clip recorded from an animal (e.g.
a dog), our framework uses reinforcement learning to train a control policy
that enables a robot to imitate the motion in the real world. Then, by simply
providing the system with different reference motions, we are able to train a
quadruped robot to perform a diverse set of agile behaviors, ranging from fast
walking gaits to dynamic hops and turns. The policies are trained primarily in
simulation, and then transferred to the real world using a latent space
adaptation technique, which is able to efficiently adapt a policy using only a
few minutes of data from the real robot.

<!--more-->

{% include youtubePlayer.html id="lKYh6uuCwRY" %}

## Framework

Our framework consists of three main components: motion retargeting, motion
imitation, and domain adaptation. 1) First, given a reference motion, the
motion retargeting stage maps the motion from the original animal’s morphology
to the robot’s morphology. 2) Next, the motion imitation stage uses the
retargeted reference motion to train a policy for imitating the motion in
simulation. 3) Finally, the domain adaptation stage transfers the policy from
simulation to a real robot via a sample efficient domain adaptation process. We
apply this framework to learn a variety of agile locomotion skills for a
<a href="http://www.unitree.cc/e/action/ShowInfo.php?classid=6&id=1">Laikago</a>
quadruped robot.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/laikago/01_overview.gif" width=""><br>
<i>
The framework consists of three stages: motion retargeting, motion imitation,
and domain  adaptation. It receives as input motion data recorded from an
animal, and outputs a control  policy that enables a robot to reproduce the
motion in the real world.
</i>
</p>

### Motion Retargeting

An animal’s body is generally quite different from a robot’s body. So before
the robot can imitate the animal’s motion, we must first map the motion to the
robot’s body. The goal of the retargeting process is to construct a reference
motion for the robot that captures the important characteristics of the
animal’s motion. To do this, we first identify a set of source keypoints on the
animal’s body, such as the hips and the feet. Then, corresponding target
keypoints are specified on the robot’s body.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/laikago/02_keypoints_dog.png" height="280">
<img src="https://bair.berkeley.edu/static/blog/laikago/02_keypoints_robot.png" height="280">
<br>
<i>
Inverse-kinematics (IK) is used to retarget mocap clips recorded from a real
dog (left) to the robot (right). Corresponding pairs of keypoints (red) are
specified on the dog and robot’s bodies,  and then IK is used to compute a pose
for the robot that tracks the keypoints.
</i>
</p>

Next, inverse-kinematics is used to construct a reference motion for the robot
that tracks the corresponding keypoints from the animal at every timestep.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/laikago/03_retarget_pace.gif" height="170">
<img src="https://bair.berkeley.edu/static/blog/laikago/04_retarget_spin.gif" height="170">
<br>
<i>
Inverse-kinematics is used to retarget mocap clips recorded from a dog to the robot.
</i>
</p>

### Motion Imitation

After retargeting the reference motion to the robot, the next step is to train
a control policy to imitate the retargeted motion. But reinforcement learning
algorithms can take a long time to learn an effective policy, and directly
training on a real robot can be fairly dangerous (both for the robot and its
human companions). So, we instead opt to perform most of the training in the
comforts of simulation, and then transfer the learned policy to the real world
using more sample efficient adaptation techniques. All simulations are
performed using <a href="https://pybullet.org/">PyBullet</a>.

The policy $\pi(\mathbf{a} | \mathbf{s}, \mathbf{g})$, takes as input a state
$\mathbf{s}$, which represents the configuration of the robot’s body, and a
goal $\mathbf{g}$, which specifies target poses from the reference motion that
the robot is to imitate.  It then outputs an action $\mathbf{a}$, which
specifies target angles for PD controllers at each of the robot’s joints. To
train the policy to imitate a reference motion, we use a reward function that
encourages the robot to minimize the difference between the pose of the
reference motion $\hat{\mathbf{q}}_t$ and the pose of the simulated character
$\mathbf{q}_t$ at every timestep $t$,

$$
r_t = \exp \Big[ - \| \hat{\mathbf{q}}_t - \mathbf{q}_t \|^2 \Big]
$$

By simply using different reference motions in the reward function, we can
train a simulated robot to imitate a variety of different skills.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/laikago/05_sim_pace.gif" height="170">
<img src="https://bair.berkeley.edu/static/blog/laikago/06_sim_spin.gif" height="170">
<br>
<i>
Reinforcement learning is used to train a simulated robot to imitate the
retargeted reference motions.
</i>
</p>

### Domain Adaptation

Since simulators generally provide only a coarse approximation of the real
world, policies trained in simulation often perform fairly poorly when deployed
on a real robot. Therefore, to transfer a policy trained in simulation to the
real world, we use a sample efficient domain adaptation techniques that can
adapt the policy to the real world using only a small number of trials on the
real robot. To do this, we first apply
<a href="https://xbpeng.github.io/projects/SimToReal/index.html">domain randomization</a> during training in simulation, which randomly varies the
dynamics parameters, such as mass and friction. The dynamics parameters are
then also collected into a vector $\mu$ and encoded into a latent presentation
$\mathbf{z}$ by an encoder $E(\mathbf{z} | \mu)$. The latent encoding  is
passed as an additional input to the policy $\pi(\mathbf{a} | \mathbf{s},
\mathbf{g}, \mathbf{z})$.


<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/laikago/07_policy.png" width="500">
<br>
<i>
The dynamics parameters  of the simulation are varied during training, and also
encoded into a latent representation  that is provided as an additional input
to the policy.
</i>
</p>

When transferring the policy to a real robot, we remove the encoder and
directly search for a $\mathbf{z}$ that maximizes the robot’s rewards in the
real world.  This is done using
<a href="https://xbpeng.github.io/projects/AWR/index.html">advantage weighted regression</a>,
a simple off-policy reinforcement learning algorithm. In our experiments, this
technique is often able to adapt a policy to the real world with less than 50
trials, which corresponds to roughly 8 minutes of real-world data.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/laikago/08_adaptation_pace.gif" width=""><br>
<img src="https://bair.berkeley.edu/static/blog/laikago/09_adaptation_spin.gif" width="">
<br>
<i>
Comparison of policies before and after adaptation on the real robot. Before
adaptation, the robot is prone to falling. But after adaptation, the policies
are able to more consistently execute the desired skills.
</i>
</p>

## Results

Our framework is able to train a robot to imitate various locomotion skills
from a dog, including different walking gaits, such as pacing and trotting, as
well as a fast spinning motion. By simply playing the forwards walking motions
backwards, we are also able to train the robot to walk backwards.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/laikago/10_real_pace.gif" width="320">
<img src="https://bair.berkeley.edu/static/blog/laikago/11_real_trot.gif" width="320"><br>
<img src="https://bair.berkeley.edu/static/blog/laikago/12_real_spin.gif" width="320">
<img src="https://bair.berkeley.edu/static/blog/laikago/13_real_backward_trot.gif" width="320">
<br>
<i>
Laikago imitating various skills from a dog.
</i>
</p>

In addition to imitating motions from real dogs, we can also imitate
artist-animated keyframe motion, including a dynamic hop-turn:

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/laikago/14_real_sidesteps.gif" width="360">
<img src="https://bair.berkeley.edu/static/blog/laikago/14_real_turn.gif" width="360"><br>
Hop Turn
<img src="https://bair.berkeley.edu/static/blog/laikago/15_real_hopturn.gif" width="">
<br>
<i>
Skills learned by imitating artist-animated keyframe motions.
</i>
</p>

We also compared the learned policies with the manually-designed controllers
provided by the manufacturer. Our policies are able to learn faster gaits.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/laikago/16_comp_trot.gif" width=""><br>
<br>
<i>
Comparison of learned trotting gait with the built-in gait provided by the
manufacturer.
</i>
</p>

Overall, our system has been able to reproduce a fairly diverse corpus of
behaviors with a quadruped robot. However, due to hardware and algorithmic
limitations, we have not been able to imitate more dynamic motions such as
running and jumping. The learned policies are also not as robust as the best
manually-designed controllers. Exploring techniques for further improving the
agility and robustness of these learned policies could be a valuable step
towards more complex real-world applications. Extending this framework to learn
<a href="https://xbpeng.github.io/projects/SFV/index.html">skills from videos</a>
would also be an exciting direction, which can substantially increase the
volume of data from which robots can learn from.

To learn more,
<a href="https://xbpeng.github.io/projects/Robotic_Imitation/index.html">check out the paper and code</a>.

We would like to thank Erwin Coumans, Tingnan Zhang, Tsang-Wei Lee, Jie Tan,
Sergey Levine, Byron David, Thinh Nguyen, Gus Kouretas, Krista Reymann, and
Bonny Ho for all their support and contribution to this work. This project was
done in collaboration with Google Brain.
